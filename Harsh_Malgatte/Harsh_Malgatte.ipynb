{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9626f077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying SerpApi\n",
      "Success with SerpApi!\n",
      "\n",
      "Results:\n",
      "[\n",
      "  {\n",
      "    \"title\": \"Long short-term memory\",\n",
      "    \"link\": \"https://en.wikipedia.org/wiki/Long_short-term_memory\",\n",
      "    \"snippet\": \"The long short-term memory (LSTM) cell can process data sequentially and keep its hidden state through time. An LSTM unit is typically composed of a cell and ...\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"What is LSTM - Long Short Term Memory?\",\n",
      "    \"link\": \"https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory/\",\n",
      "    \"snippet\": \"Long Short-Term Memory (LSTM) is an enhanced version of the Recurrent Neural Network (RNN) designed by Hochreiter and Schmidhuber.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"What is LSTM? Introduction to Long Short-Term Memory\",\n",
      "    \"link\": \"https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm/\",\n",
      "    \"snippet\": \"LSTM (Long Short-Term Memory) is a recurrent neural network (RNN) architecture widely used in Deep Learning. It excels at capturing long-term dependencies.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"What is LSTM (Long Short Term Memory)?\",\n",
      "    \"link\": \"https://www.youtube.com/watch?v=b61DPVFX03I\",\n",
      "    \"snippet\": \"It allows a neural network to remember the stuff that it needs to keep hold of context but also to forget the stuff that well is no longer applicable.\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"A Gentle Introduction to Long Short-Term Memory ...\",\n",
      "    \"link\": \"https://www.machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/\",\n",
      "    \"snippet\": \"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wikipedia\n",
    "from serpapi import GoogleSearch\n",
    "from duckduckgo_search import DDGS\n",
    "import json\n",
    "\n",
    "def search_serpapi(query, num_results=5):\n",
    "    print(\"Trying SerpApi\")\n",
    "    api_key = os.getenv(\"SERPAPI_API_KEY\")\n",
    "    \n",
    "\n",
    "    params = {\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"api_key\": api_key,\n",
    "    }\n",
    "\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "\n",
    "    organic_results = results.get(\"organic_results\", [])\n",
    "    if not organic_results:\n",
    "        return []\n",
    "\n",
    "    formatted_results = [\n",
    "        {\"title\": item.get(\"title\"), \"link\": item.get(\"link\"), \"snippet\": item.get(\"snippet\")}\n",
    "        for item in organic_results[:num_results]\n",
    "    ]\n",
    "    return formatted_results\n",
    "\n",
    "\n",
    "def search_ddg(query, num_results=5):\n",
    "    print(\"SerpApi failed. Trying DuckDuckGo\")\n",
    "    with DDGS() as ddgs:\n",
    "        results = list(ddgs.text(query, max_results=num_results))\n",
    "\n",
    "    if not results:\n",
    "        return []\n",
    "\n",
    "    formatted_results = [\n",
    "        {\"title\": item.get(\"title\"), \"link\": item.get(\"href\"), \"snippet\": item.get(\"body\")}\n",
    "        for item in results\n",
    "    ]\n",
    "    return formatted_results\n",
    "\n",
    "\n",
    "def search_wikipedia(query, num_results=3):\n",
    "    print(\"DuckDuckGo failed. Trying Wikipedia...\")\n",
    "    try:\n",
    "        page_titles = wikipedia.search(query, results=num_results)\n",
    "        if not page_titles:\n",
    "            return []\n",
    "\n",
    "        formatted_results = []\n",
    "        for title in page_titles:\n",
    "            try:\n",
    "                page = wikipedia.page(title, auto_suggest=False)\n",
    "                summary = wikipedia.summary(title, sentences=2)\n",
    "                formatted_results.append({\n",
    "                    \"title\": page.title,\n",
    "                    \"link\": page.url,\n",
    "                    \"snippet\": summary\n",
    "                })\n",
    "            except wikipedia.exceptions.DisambiguationError:\n",
    "                print(f\"'{title}' was ambiguous, skipping.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Couldn't get page for '{title}': {e}\")\n",
    "\n",
    "        return formatted_results\n",
    "    except Exception as e:\n",
    "        print(f\"Something went wrong with Wikipedia search: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def robust_web_search(query, num_results=5):\n",
    "    try:\n",
    "        results = search_serpapi(query, num_results)\n",
    "        if results:\n",
    "            print(\"Success with SerpApi!\")\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"SerpApi error: {e}\")\n",
    "\n",
    "    try:\n",
    "        results = search_ddg(query, num_results)\n",
    "        if results:\n",
    "            print(\"Success with DuckDuckGo!\")\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"DuckDuckGo error: {e}\")\n",
    "\n",
    "    try:\n",
    "        results = search_wikipedia(query, num_results)\n",
    "        if results:\n",
    "            print(\"Success with Wikipedia!\")\n",
    "            return results\n",
    "    except Exception as e:\n",
    "        print(f\"Wikipedia error: {e}\")\n",
    "\n",
    "    print(\"Sorry, all search methods failed.\")\n",
    "    return []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = \"What are LSTMs ?\"\n",
    "    search_results = robust_web_search(search_query)\n",
    "\n",
    "    if search_results:\n",
    "        print(\"\\nResults:\")\n",
    "        print(json.dumps(search_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cecc71b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying SerpApi\n",
      "Success with SerpApi!\n",
      "\n",
      "Starting to scrape websites\n",
      "Scraping: https://en.wikipedia.org/wiki/Long_short-term_memory\n",
      "Scraping: https://www.geeksforgeeks.org/deep-learning/deep-learning-introduction-to-long-short-term-memory/\n",
      "Scraping: https://www.analyticsvidhya.com/blog/2021/03/introduction-to-long-short-term-memory-lstm/\n",
      "Scraping: https://www.machinelearningmastery.com/gentle-introduction-long-short-term-memory-networks-experts/\n",
      "Scraping: https://developer.nvidia.com/discover/lstm\n",
      "Finished scraping.\n",
      "\n",
      "Next, building the RAG system\n",
      "1. Splitting the text into chunks...\n",
      "Made 292 chunks of text.\n",
      "2. Loading the embedding model\n",
      "3. Storing chunks in a vector database\n",
      "Vector database is ready.\n",
      "4. Setting up the LLM\n",
      "5. Creating the final QA chain\n",
      "\n",
      "Okay, let's ask the agent a question.\n",
      "\n",
      "Question: What are LSTMs and how do they work?\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harshsiddharthmalgatte/data_doritos/.conda/lib/python3.11/site-packages/langchain_google_genai/llms.py:44: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  for field_name, field_info in self.model_fields.items():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Answer from RAG Agent ---\n",
      "Based on the provided text, LSTMs are a type of recurrent neural network.  They use a self-connected hidden layer containing memory cells and gate units.  The text explains that understanding their operation is best done through analogy rather than equations, but it doesn't detail the specifics of that analogy or the workings of the memory cells and gate units.  The text mentions applications such as speech recognition, image captioning, and time series forecasting.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- RAG Agent Code ---\n",
    "\n",
    "def scrape_websites(urls):\n",
    "    print(\"\\nStarting to scrape websites\")\n",
    "    full_text = \"\"\n",
    "    for url in urls:\n",
    "        try:\n",
    "            print(f\"Scraping: {url}\")\n",
    "            response = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):\n",
    "                element.decompose()\n",
    "            \n",
    "            text = soup.get_text(separator='\\n', strip=True)\n",
    "            full_text += f\"\\n\\n--- Content from {url} ---\\n\\n{text}\"\n",
    "        except requests.RequestException:\n",
    "            print(f\"Skipping {url}, couldn't fetch it.\")\n",
    "        except Exception:\n",
    "            print(f\"Something else went wrong with {url}.\")\n",
    "            \n",
    "    print(\"Finished scraping.\")\n",
    "    return full_text\n",
    "\n",
    "def create_rag_agent(text_corpus):\n",
    "    print(\"\\nNext, building the RAG system\")\n",
    "    if not text_corpus or text_corpus.isspace():\n",
    "        print(\"Text is empty, can't build the agent.\")\n",
    "        return None\n",
    "\n",
    "    print(\"1. Splitting the text into chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, \n",
    "        chunk_overlap=100,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text_corpus)\n",
    "    print(f\"Made {len(chunks)} chunks of text.\")\n",
    "\n",
    "    print(\"2. Loading the embedding model\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    print(\"3. Storing chunks in a vector database\")\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    print(\"Vector database is ready.\")\n",
    "\n",
    "    print(\"4. Setting up the LLM\")\n",
    "    load_dotenv()\n",
    "    llm = GoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "    print(\"5. Creating the final QA chain\")\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_store.as_retriever(search_kwargs={\"k\": 3}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    my_topic = \"What are LSTMs?\"\n",
    "    search_results = robust_web_search(my_topic, num_results=5)\n",
    "\n",
    "    if not search_results:\n",
    "        print(\"Couldn't get any search results.\")\n",
    "    else:\n",
    "        urls_to_scrape = [res['link'] for res in search_results if res.get('link')]\n",
    "        \n",
    "        scraped_content = scrape_websites(urls_to_scrape)\n",
    "        \n",
    "        rag_agent = create_rag_agent(scraped_content)\n",
    "        \n",
    "        if rag_agent:\n",
    "            print(\"\\nOkay, let's ask the agent a question.\")\n",
    "            rag_question = \"What are LSTMs and how do they work?\"\n",
    "            \n",
    "            print(f\"\\nQuestion: {rag_question}\\n\")\n",
    "            \n",
    "            response = rag_agent({\"query\": rag_question})\n",
    "            \n",
    "            print(\"--- Answer from RAG Agent ---\")\n",
    "            print(response[\"result\"])\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
