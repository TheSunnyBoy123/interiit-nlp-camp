# -*- coding: utf-8 -*-
"""assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WrIAlLcO1_i9JloCSw8DQzTHmdTfG0Pr

#Step 1
"""

import time
import requests
from bs4 import BeautifulSoup

# --- logging (optional but helpful) ---
import logging
logging.basicConfig(level=logging.INFO, format="%(levelname)s:%(name)s:%(message)s")
LOG = logging.getLogger("search-agents")

# --- small constants used by all providers ---
DEFAULT_TIMEOUT = 10  # seconds
USER_AGENT = (
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36"
)

from dataclasses import dataclass

@dataclass
class SearchResult:
    title: str
    url: str
    snippet: str
    provider: str

SearchResult(
    title="Retrieval-Augmented Generation - Wikipedia",
    url="https://en.wikipedia.org/wiki/Retrieval-augmented_generation",
    snippet="RAG is a technique that combines ...",
    provider="wikipedia"
)

from typing import List, Protocol

class SearchProvider(Protocol):
    name: str
    def search(self, query: str, k: int = 10) -> List[SearchResult]:
        ...

class ProviderError(Exception):
    """Generic provider failure"""

class RateLimitError(ProviderError):
    """Special case: too many requests (HTTP 429)"""

import time
from urllib.parse import urlparse
from typing import Optional, Dict, Tuple

def exponential_backoff(attempt: int, base: float = 0.8, cap: float = 8.0) -> float:
    return min(cap, base * (2 ** (attempt - 1)))

def normalize_url(u: str) -> str:
    try:
        p = urlparse(u)
        return p._replace(fragment="").geturl()
    except Exception:
        return u

def dedupe(results: List[SearchResult]) -> List[SearchResult]:
    seen, out = set(), []
    for r in results:
        key = normalize_url(r.url)
        if key not in seen:
            out.append(r)
            seen.add(key)
    return out

_CACHE: Dict[Tuple[str, int], List[SearchResult]] = {}

def cache_get(query: str, k: int) -> Optional[List[SearchResult]]:
    return _CACHE.get((query, k))

def cache_put(query: str, k: int, results: List[SearchResult]) -> None:
    if len(_CACHE) > 100:
        _CACHE.clear()
    _CACHE[(query, k)] = results

"""#1st web search agent"""

class WikipediaProvider:
    'A search agent using the public api of wikipedia.'

    name = "wikipedia"

    def __init__(self, language: str="en"):
        self.language = "en"
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": USER_AGENT})
        self.endpoint = f"https://www.wikipedia.org/w/api.php" #endpoint for English Wikipedia API"


    def search(self, query: str, k: int =10) -> list[SearchResult]:
        'search the query on wikipedia and return k search results.'
        params = {
            "action": "query",
            "list": "search",
            "format": "json",
            "srlimit": min(k, 50),
            "srsearch": query
        }

        for attempt in range(1, 3):
            try:

                resp = self.session.get(self.endpoint, params=params, timeout=DEFAULT_TIMEOUT)

                if resp.status_code == 429:
                    raise RateLimitError("Wikipedia rate limited (HTTP 429)")

                resp.raise_for_status()

                # Parse JSON → Python dict
                data = resp.json()

                # Navigate to the search result list in the JSON structure
                items = data.get("query", {}).get("search", [])

                results: list[SearchResult] = []

                # Build uniform results
                for it in items[:k]:
                    title = it.get("title", "")

                    page_url_title = title.replace(" ", "_")
                    url = f"https://{self.language}.wikipedia.org/wiki/{page_url_title}"

                    snippet_html = it.get("snippet", "")
                    snippet = BeautifulSoup(snippet_html, "html.parser").get_text(" ", strip=True)

                    results.append(SearchResult(
                        title=title,
                        url=url,
                        snippet=snippet,
                        provider=self.name
                    ))

                # If we got at least one result, return it
                if results:
                    return results

                # Otherwise, treat "no results" as a failure (so the orchestrator can fallback)
                raise ProviderError("Wikipedia returned 0 results")

            except RateLimitError as e:
                LOG.warning("%s (attempt %d): %s", self.name, attempt, str(e))
                time.sleep(exponential_backoff(attempt))

            except (requests.RequestException, ValueError) as e:
                LOG.warning("%s (attempt %d): %s", self.name, attempt, str(e))
                time.sleep(exponential_backoff(attempt))

        raise ProviderError("WikipediaProvider failed after retries")

if __name__ == "__main__":
    from pprint import pprint

    wiki = WikipediaProvider(language="en")
    results = wiki.search("retrieval augmented generation", k=5)
    for r in results:
        pprint(vars(r))

"""#2nd web search agent"""

class DuckDuckGoProvider:
  'A search agent using the DuckDuckGo API.'
  name = "duckduckgo"
  def __init__ (self):
    self.session = requests.Session()
    self.session.headers.update({"User-Agent": USER_AGENT})
    self.endpoint = "https://api.duckduckgo.com/"

  def search (self, query, k=10) -> list[SearchResult]:
    params = {
            "q": query,
            "format": "json",
            "no_html": 1,
            "skip_disambig": 1
        }
    try:
            resp = self.session.get(self.endpoint, params=params, timeout=5)
            resp.raise_for_status()
            data = resp.json()

            from pprint import pprint
            pprint(data)

            results = []
            for item in data.get("RelatedTopics", []):
                if "Text" in item and "FirstURL" in item:
                    results.append(SearchResult(
                        title=item.get("Text", ""),
                        url=item.get("FirstURL", ""),
                        snippet="",
                        provider="DuckDuckGo"
))
                if len(results) >= k:
                    break

            return results

    except Exception as e:
            print(f"[DuckDuckGo] search failed: {e}")
            return []

if __name__ == "__main__":
    print("\n--- DuckDuckGo Test ---")
    ddg = DuckDuckGoProvider()
    results = ddg.search("IIT kanpur", k=5)
    for r in results:
        print(vars(r))

"""# 3rd web search agent"""

class HackerNewsProvider:
    """
    A simple search provider using Hacker News Algolia API.
    No API key required.
    """

    name = "hackernews"

    def __init__(self):
        self.endpoint = "http://hn.algolia.com/api/v1/search"
        self.session = requests.Session()
        self.session.headers.update({"User-Agent": USER_AGENT})

    def search(self, query: str, k: int = 10) -> list[SearchResult]:
        params = {
            "query": query,
            "tags": "story",   # only search story titles
            "hitsPerPage": k
        }

        try:
            resp = self.session.get(self.endpoint, params=params, timeout=10)
            resp.raise_for_status()
            data = resp.json()

            results = []
            for hit in data.get("hits", []):
                title = hit.get("title") or "(no title)"
                url = hit.get("url") or f"https://news.ycombinator.com/item?id={hit.get('objectID')}"
                snippet = hit.get("story_text") or ""

                results.append(SearchResult(
                    title=title,
                    url=url,
                    snippet=snippet,
                    provider=self.name
                ))

            if not results:
                raise ProviderError("HackerNews returned 0 results")

            return results

        except Exception as e:
            raise ProviderError(f"[HackerNews] search failed: {e}")

if __name__ == "__main__":
    from pprint import pprint

    hn = HackerNewsProvider()
    results = hn.search("IIT Kanpur", k=5)
    for r in results:
        pprint(vars(r))

class SearchAgent:
    """
    Orchestrates multiple search providers.
    Tries each provider in order and collects results.
    """

    def __init__(self, providers: list):
        """
        providers: list of provider instances, e.g.
        [WikipediaProvider(), DuckDuckGoProvider(), HackerNewsProvider()]
        """
        self.providers = providers

    def search(self, query: str, k_per_provider: int = 5) -> list:
        """
        Runs the query on each provider in order.
        Returns a combined list of results.
        """
        all_results = []

        for provider in self.providers:
            try:
                results = provider.search(query, k=k_per_provider)
                print(f"[{provider.name}] returned {len(results)} results")
                all_results.extend(results)

            except Exception as e:
                print(f"[{provider.name}] search failed: {e}")
                continue  # fallback to next provider

        if not all_results:
            print("No results found from any provider.")
        return all_results

if __name__ == "__main__":
    from pprint import pprint

    # create providers
    wiki = WikipediaProvider()
    ddg = DuckDuckGoProvider()
    hn = HackerNewsProvider()

    # create orchestrator
    agent = SearchAgent([wiki, ddg, hn])

    # run a query
    query = "IIT Kanpur campus life"
    results = agent.search(query, k_per_provider=3)  # 3 results per provider

    # print results
    for r in results:
        pprint(vars(r))



"""# NEXT PART"""

from __future__ import annotations
import re
import time
from dataclasses import dataclass
from typing import Iterable, List, Optional
import requests
from bs4 import BeautifulSoup

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/115 Safari/537.36"
)

# --- data containers ---------------------------------

@dataclass
class Doc:
    """One scraped document."""
    url: str
    title: str
    text: str

@dataclass
class Chunk:
    """One chunk cut from a document."""
    url: str
    title: str
    chunk_id: str   # e.g., f"{url}#chunk-3"
    text: str
    index: int

# --- scraper -----------------------------------------

class ScrapeError(Exception):
    pass

class SimpleScraper:
    """
    Minimal, polite HTML scraper:
      - sets a real User-Agent
      - times out if site is slow
      - rejects non-HTML content
      - strips scripts/menus/footers
    """

    def __init__(self,
                 timeout_sec: int = 12,
                 max_bytes: int = 2_000_000,
                 allowed_types: tuple[str, ...] = ("text/html", "application/xhtml+xml"),
                 sleep_between: float = 0.5):
        self.timeout_sec = timeout_sec
        self.max_bytes = max_bytes
        self.allowed_types = allowed_types
        self.sleep_between = sleep_between

        self.session = requests.Session()
        self.session.headers.update({"User-Agent": USER_AGENT})

    def fetch(self, url: str) -> str:
        """Download HTML as text with basic validation."""
        try:
            resp = self.session.get(url, timeout=self.timeout_sec, stream=True)
            resp.raise_for_status()

            # content-type check
            ctype = (resp.headers.get("Content-Type") or "").split(";")[0].strip().lower()
            if not any(ctype.startswith(t) for t in self.allowed_types):
                raise ScrapeError(f"Unsupported content-type: {ctype!r}")

            # size guard (read at most max_bytes)
            content = resp.raw.read(self.max_bytes, decode_content=True)
            try:
                html = content.decode(resp.encoding or "utf-8", errors="ignore")
            except Exception:
                html = content.decode("utf-8", errors="ignore")

            return html

        except requests.RequestException as e:
            raise ScrapeError(f"network error: {e}") from e

    def extract_text(self, html: str) -> tuple[str, str]:
        """
        Pull a usable title + main text from HTML.
        (Not perfect, but solid enough for RAG bootstrapping.)
        """
        soup = BeautifulSoup(html, "html.parser")

        title = (soup.title.string if soup.title and soup.title.string else "").strip()

        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        for tag_name in ("header", "footer", "nav", "aside"):
            for tag in soup.find_all(tag_name):
                tag.decompose()

        main_container = soup.find("main") or soup.find("article") or soup.body or soup
        text = main_container.get_text(separator="\n")

        text = re.sub(r"[ \t]+", " ", text)
        text = re.sub(r"\n{2,}", "\n\n", text).strip()

        if len(text) < 200:
            text = soup.get_text(separator="\n")
            text = re.sub(r"[ \t]+", " ", text)
            text = re.sub(r"\n{2,}", "\n\n", text).strip()

        return title, text

    def scrape(self, url: str) -> Doc:
        """High-level: fetch → parse → Doc."""
        html = self.fetch(url)
        title, text = self.extract_text(html)
        if not text:
            raise ScrapeError("empty text after extraction")
        time.sleep(self.sleep_between)
        return Doc(url=url, title=title or url, text=text)

    def scrape_many(self, urls: Iterable[str]) -> List[Doc]:
        docs: List[Doc] = []
        for u in urls:
            try:
                docs.append(self.scrape(u))
                print(f"[scraped] {u}")
            except ScrapeError as e:
                print(f"[skip] {u} → {e}")
        return docs

# --- chunker -----------------------------------------

def chunk_text(
    text: str,
    max_words: int = 220,
    overlap_words: int = 40
) -> List[str]:
    """
    Split long text into overlapping chunks by sentences.
    - max_words: ~size of each chunk
    - overlap_words: carry context between chunks
    """
    # naive sentence split (punctuation-based)
    sentences = re.split(r"(?<=[.!?])\s+", text)
    chunks, cur, cur_len = [], [], 0

    def flush():
        nonlocal cur, cur_len
        if cur:
            chunks.append(" ".join(cur).strip())
            # start next chunk with overlap
            if overlap_words > 0:
                last_words = chunks[-1].split()[-overlap_words:]
                cur = [" ".join(last_words)]
                cur_len = len(last_words)
            else:
                cur, cur_len = [], 0

    for s in sentences:
        w = s.split()
        if cur_len + len(w) > max_words and cur:
            flush()
        cur.append(s)
        cur_len += len(w)

    if cur:
        chunks.append(" ".join(cur).strip())

    # remove accidental empties
    return [c for c in chunks if c]

def to_chunks(doc: Doc, max_words: int = 220, overlap_words: int = 40) -> List[Chunk]:
    out: List[Chunk] = []
    parts = chunk_text(doc.text, max_words=max_words, overlap_words=overlap_words)
    for i, p in enumerate(parts):
        out.append(Chunk(
            url=doc.url,
            title=doc.title,
            chunk_id=f"{doc.url}#chunk-{i}",
            text=p,
            index=i
        ))
    return out

# --- demo -------

if __name__ == "__main__":
    urls = [
        "https://en.wikipedia.org/wiki/Retrieval-augmented_generation",
        "https://en.wikipedia.org/wiki/IIT_Kanpur",
    ]

    scraper = SimpleScraper()
    docs = scraper.scrape_many(urls)

    print(f"\nScraped {len(docs)} pages. Showing titles + first 400 chars:\n")
    for d in docs:
        print("TITLE:", d.title)
        print("URL  :", d.url)
        print(d.text[:400].replace("\n", " "), "...\n")

    if docs:
        chunks = to_chunks(docs[0], max_words=200, overlap_words=40)
        print(f"\nMade {len(chunks)} chunks from first doc. Showing first 2:\n")
        for c in chunks[:2]:
            print(c.chunk_id, f"(~{len(c.text.split())} words)")
            print(c.text[:300].replace("\n", " "), "...\n")

pip install sentence-transformers faiss-cpu

# -----------------------------
# step 2: embeddings + FAISS index
# -----------------------------
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

class Embedder:
    """Wraps a sentence-transformer model to embed text into vectors."""
    def __init__(self, model_name: str = "all-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)

    def encode(self, texts: list[str]) -> np.ndarray:
        return self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)

class VectorIndex:
    """FAISS-based vector search index."""
    def __init__(self, dim: int):
        self.index = faiss.IndexFlatL2(dim)  # cosine similarity can also be done with normalization
        self.chunks: list[Chunk] = []

    def add(self, chunks: list[Chunk], embeddings: np.ndarray):
        if embeddings.shape[0] != len(chunks):
            raise ValueError("mismatch: #embeddings vs #chunks")
        self.index.add(embeddings.astype("float32"))
        self.chunks.extend(chunks)

    def search(self, query_vec: np.ndarray, top_k: int = 3) -> list[tuple[Chunk, float]]:
        D, I = self.index.search(query_vec.astype("float32"), top_k)
        out = []
        for idx, dist in zip(I[0], D[0]):
            if idx == -1:
                continue
            out.append((self.chunks[idx], float(dist)))
        return out

# --- demo usage --------------------------------------

if __name__ == "__main__":

    all_chunks: list[Chunk] = []
    for d in docs:
        all_chunks.extend(to_chunks(d))

    print(f"\nTotal chunks across docs: {len(all_chunks)}")


    embedder = Embedder()
    texts = [c.text for c in all_chunks]
    embeddings = embedder.encode(texts)

    dim = embeddings.shape[1]
    vindex = VectorIndex(dim)
    vindex.add(all_chunks, embeddings)


    query = "What is IIT Kanpur famous for?"
    qvec = embedder.encode([query])

    results = vindex.search(qvec, top_k=3)

    print(f"\nQuery: {query}\nTop chunks:\n")
    for chunk, dist in results:
        print(f"- ({dist:.2f}) {chunk.title} | {chunk.url}")
        print(chunk.text[:200].replace("\n", " "), "...\n")

pip install transformers accelerate

from transformers import pipeline

class RAGPipeline:
    """Simple retrieval-augmented generator."""
    def __init__(self, embedder: Embedder, vindex: VectorIndex, model_name="google/flan-t5-base"):
        self.embedder = embedder
        self.vindex = vindex
        self.generator = pipeline("text2text-generation", model=model_name)

    def answer(self, query: str, top_k: int = 3) -> str:
        # 1. embed query
        qvec = self.embedder.encode([query])

        # 2. retrieve top chunks
        results = self.vindex.search(qvec, top_k=top_k)
        context = "\n".join([c.text for c, _ in results])

        # 3. construct prompt
        prompt = f"Use the following context to answer the question.\n\nContext:\n{context}\n\nQuestion: {query}\nAnswer:"

        # 4. generate
        output = self.generator(prompt, max_length=200, num_return_sequences=1)[0]["generated_text"]
        return output

if __name__ == "__main__":
    rag = RAGPipeline(embedder, vindex)

    query = "What is IIT Kanpur famous for?"
    answer = rag.answer(query)

    print("\n=== RAG Answer ===")
    print(answer)

from __future__ import annotations
import re
import time
import urllib.parse
from dataclasses import dataclass
from typing import Iterable, List, Tuple
import requests
from bs4 import BeautifulSoup

@dataclass
class SearchResult:
    title: str
    url: str
    snippet: str
    provider: str


BAD_EXTENSIONS = (
    ".pdf", ".ppt", ".pptx", ".xls", ".xlsx", ".doc", ".docx",
    ".zip", ".rar", ".7z", ".mp3", ".mp4", ".avi", ".mov", ".svg", ".png", ".jpg", ".jpeg", ".gif"
)

def is_html_like(url: str) -> bool:
    """Heuristic: keep likely HTML pages, skip files/attachments."""
    parsed = urllib.parse.urlparse(url)
    path = parsed.path.lower()
    if any(path.endswith(ext) for ext in BAD_EXTENSIONS):
        return False
    return True

def same_domain(a: str, b: str) -> bool:
    return urllib.parse.urlparse(a).netloc == urllib.parse.urlparse(b).netloc

def dedupe_urls(urls: Iterable[str], max_per_domain: int = 2) -> List[str]:
    """
    Keep at most N URLs per domain to get diversity across 5 large sites.
    """
    seen = {}
    picked = []
    for u in urls:
        dom = urllib.parse.urlparse(u).netloc
        seen.setdefault(dom, 0)
        if seen[dom] < max_per_domain:
            picked.append(u)
            seen[dom] += 1
    return picked

USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/115 Safari/537.36"
)

class ScrapeError(Exception):
    pass

@dataclass
class Doc:
    url: str
    title: str
    text: str

class SimpleScraper:
    """
    Minimal, polite HTML scraper:
      - real User-Agent
      - timeouts + size cap
      - rejects non-HTML
      - strips scripts/menus/styles
    """
    def __init__(
        self,
        timeout_sec: int = 12,
        max_bytes: int = 2_000_000,
        sleep_between: float = 0.5,
        allowed_types: Tuple[str, ...] = ("text/html", "application/xhtml+xml"),
    ):
        self.timeout_sec = timeout_sec
        self.max_bytes = max_bytes
        self.sleep_between = sleep_between
        self.allowed_types = allowed_types

        self.session = requests.Session()
        self.session.headers.update({"User-Agent": USER_AGENT})

    def fetch(self, url: str) -> str:
        try:
            resp = self.session.get(url, timeout=self.timeout_sec, stream=True)
            resp.raise_for_status()

            # basic content-type check
            ctype = (resp.headers.get("Content-Type") or "").split(";")[0].strip().lower()
            if not any(ctype.startswith(t) for t in self.allowed_types):
                raise ScrapeError(f"unsupported content-type: {ctype!r}")

            raw = resp.raw.read(self.max_bytes, decode_content=True)
            html = raw.decode(resp.encoding or "utf-8", errors="ignore")
            return html
        except requests.RequestException as e:
            raise ScrapeError(f"network error: {e}") from e
        except Exception as e:
            raise ScrapeError(f"decode/other error: {e}") from e

    def extract_text(self, html: str) -> Tuple[str, str]:
        soup = BeautifulSoup(html, "html.parser")

        # title
        title = (soup.title.string if soup.title and soup.title.string else "").strip()

        # remove noise
        for tag in soup(["script", "style", "noscript"]):
            tag.decompose()
        for tn in ("header", "footer", "nav", "aside"):
            for t in soup.find_all(tn):
                t.decompose()


        container = soup.find("main") or soup.find("article") or soup.body or soup
        text = container.get_text(separator="\n")

        # normalize whitespace
        text = re.sub(r"[ \t]+", " ", text)
        text = re.sub(r"\n{2,}", "\n\n", text).strip()

        if len(text) < 200:  # fallback to whole page
            text = soup.get_text(separator="\n")
            text = re.sub(r"[ \t]+", " ", text)
            text = re.sub(r"\n{2,}", "\n\n", text).strip()

        return title, text

    def scrape(self, url: str) -> Doc:
        html = self.fetch(url)
        title, text = self.extract_text(html)
        if not text:
            raise ScrapeError("empty text after extraction")
        time.sleep(self.sleep_between)  # be polite
        return Doc(url=url, title=title or url, text=text)

    def scrape_many(self, urls: Iterable[str]) -> List[Doc]:
        out: List[Doc] = []
        for u in urls:
            try:
                out.append(self.scrape(u))
                print(f"[scraped] {u}")
            except ScrapeError as e:
                print(f"[skip] {u} → {e}")
        return out

def pick_top_urls(results: List[SearchResult], max_sites: int = 5) -> List[str]:
    """
    From a mixed list of search results, choose up to `max_sites`
    high-quality, HTML-like pages with domain diversity.
    """
    filtered = [r.url for r in results if r.url and is_html_like(r.url)]

    seen = set()
    unique = []
    for u in filtered:
        if u not in seen:
            unique.append(u)
            seen.add(u)

    diverse = dedupe_urls(unique, max_per_domain=1)

    return diverse[:max_sites]


if __name__ == "__main__":
    mock_results = [
        SearchResult("RAG - Wikipedia", "https://en.wikipedia.org/wiki/Retrieval-augmented_generation", "", "wikipedia"),
        SearchResult("IIT Kanpur - Wikipedia", "https://en.wikipedia.org/wiki/IIT_Kanpur", "", "wikipedia"),
        SearchResult("Blog post about RAG", "https://example.com/blog/rag-guide", "", "duckduckgo"),
        SearchResult("Some PDF (should be filtered)", "https://example.com/notes.pdf", "", "duckduckgo"),
        SearchResult("Hacker News thread", "https://news.ycombinator.com/item?id=123456", "", "hackernews"),
    ]

    urls = pick_top_urls(mock_results, max_sites=5)
    print("Picked URLs:")
    for u in urls:
        print(" -", u)

    scraper = SimpleScraper()
    docs = scraper.scrape_many(urls)

    print(f"\nScraped {len(docs)} pages. Showing first 300 chars each:\n")
    for d in docs:
        print("TITLE:", d.title)
        print("URL  :", d.url)
        print(d.text[:300].replace("\n", " "), "...\n")

# Chunker

from typing import List

def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """
    Split text into overlapping chunks for embedding + retrieval.
    """
    words = text.split()
    chunks = []
    i = 0
    while i < len(words):
        chunk = words[i : i + chunk_size]
        chunks.append(" ".join(chunk))
        i += chunk_size - overlap
    return chunks

all_chunks = []
for d in docs:
    for ch in chunk_text(d.text):
        all_chunks.append({"url": d.url, "title": d.title, "text": ch})

print(f"Total chunks: {len(all_chunks)}")

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# load embedding model
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# encode chunks
embeddings = embedder.encode([c["text"] for c in all_chunks], show_progress_bar=True)
embeddings = np.array(embeddings, dtype="float32")

# build FAISS index
d = embeddings.shape[1]  # embedding dimension
index = faiss.IndexFlatL2(d)
index.add(embeddings)

print("FAISS index built:", index.ntotal, "vectors")

from openai import OpenAI
client = OpenAI()

def retrieve(query: str, k: int = 3):
    q_emb = embedder.encode([query]).astype("float32")
    D, I = index.search(q_emb, k)
    return [all_chunks[i] for i in I[0]]

def rag_answer(query: str, k: int = 3):
    ctx = retrieve(query, k)
    context_text = "\n\n".join([c["text"] for c in ctx])

    prompt = f"""
You are a helpful assistant. Use the following context to answer:

Context:
{context_text}

Question: {query}
Answer:
"""
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )
    return resp.choices[0].message.content.strip()

# --- demo
query = "What is retrieval-augmented generation?"
print(rag_answer(query))