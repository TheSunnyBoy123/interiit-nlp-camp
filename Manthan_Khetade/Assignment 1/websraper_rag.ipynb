{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c62e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_community.tools import DuckDuckGoSearchRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import Tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6aa8c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87258d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GROQ_API_KEY'] = 'entergroqapikey'\n",
    "os.environ['TAVILY_API_KEY'] = 'entertavilyapikey'\n",
    "os.environ['USER_AGENT'] = 'StateGraphRAGAgent/1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f9450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    query: str\n",
    "    search_results: dict\n",
    "    current_search_tool: str\n",
    "    rag_results: str\n",
    "    final_answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92af0a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGpipeline:\n",
    "    def __init__(self):\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "            )\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=100\n",
    "        )\n",
    "\n",
    "        self.vector_store = None\n",
    "\n",
    "    def add_text_documents(self, text: str):\n",
    "        from langchain.schema import Document\n",
    "        doc = Document(page_content=text)\n",
    "        return self.add_documents([doc])\n",
    "\n",
    "    def add_documents(self, docs):\n",
    "        chunks = self.text_splitter.split_documents(docs)\n",
    "\n",
    "        if not self.vector_store:\n",
    "            self.vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
    "        else:\n",
    "            new_vector_store = FAISS.from_documents(chunks, self.embeddings)\n",
    "            self.vector_store.merge_from(new_vector_store)\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 5, filter_type: str = None):\n",
    "        if not self.vector_store:\n",
    "            return \"\"\n",
    "        docs = self.vector_store.similarity_search(query, k=k)\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "rag_pipeline = RAGpipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a69fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(\n",
    "    model_name=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    api_key=os.getenv('GROQ_API_KEY')\n",
    ")\n",
    "\n",
    "tavily_search = TavilySearch(max_results=5)\n",
    "duckduckgo_search = DuckDuckGoSearchRun(max_results=5)\n",
    "wikipedia_search = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c197d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_query(state):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, HumanMessage):\n",
    "        state[\"query\"] = last_message.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bc95a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tavily_search(state):\n",
    "    try:\n",
    "        results = tavily_search.run(state[\"query\"])\n",
    "        state[\"search_results\"][\"tavily\"] = results\n",
    "        state[\"current_search_tool\"] = \"tavily\"\n",
    "        print(f\"Tavily search successful for: {state['query']}\")\n",
    "    except Exception as e:\n",
    "        state[\"search_results\"][\"tavily\"] = None\n",
    "        state[\"current_search_tool\"] = \"tavily\"\n",
    "        print(f\"Tavily search failed: {str(e)}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25b6dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Duckduckgo_search(state):\n",
    "    try:\n",
    "        results = duckduckgo_search.run(state[\"query\"])\n",
    "        state[\"search_results\"][\"duckduckgo\"] = results\n",
    "        state[\"current_search_tool\"] = \"duckduckgo\"\n",
    "        print(f\"DuckDuckGo search successful for: {state['query']}\")\n",
    "    except Exception as e:\n",
    "        state[\"search_results\"][\"duckduckgo\"] = None\n",
    "        state[\"current_search_tool\"] = \"duckduckgo\"\n",
    "        print(f\"DuckDuckGo search failed: {str(e)}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5969bc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wikipedia_search(state):\n",
    "    try:\n",
    "        results = wikipedia_search.run(state[\"query\"])\n",
    "        state[\"search_results\"][\"wikipedia\"] = results\n",
    "        state[\"current_search_tool\"] = \"wikipedia\"\n",
    "        print(f\"Wikipedia search successful for: {state['query']}\")\n",
    "    except Exception as e:\n",
    "        state[\"search_results\"][\"wikipedia\"] = None\n",
    "        state[\"current_search_tool\"] = \"wikipedia\"\n",
    "        print(f\"Wikipedia search failed: {str(e)}\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e52005ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_search_success(state):\n",
    "    current_tool = state[\"current_search_tool\"]\n",
    "    results = state[\"search_results\"].get(current_tool)\n",
    "        \n",
    "    if results:\n",
    "        return \"success\"\n",
    "    return \"failure\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42deca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls_from_results(search_results, max_sites=5):\n",
    "    scraped_content = []\n",
    "    urls = []\n",
    "        \n",
    "    for tool_name, results in search_results.items():\n",
    "        if results:\n",
    "            if isinstance(results, dict) and 'results' in results:\n",
    "                for item in results['results']:\n",
    "                    if 'url' in item:\n",
    "                        urls.append(item['url'])\n",
    "\n",
    "            elif isinstance(results, str):\n",
    "                url_pattern = r'https?://[^\\s<>\"{}|\\\\^`\\[\\]]+'\n",
    "                found_urls = re.findall(url_pattern, results)\n",
    "                for url in found_urls:\n",
    "                    urls.append(url)\n",
    "    \n",
    "    \n",
    "    if not urls:\n",
    "        print(\"❌ No URLs found to scrape\")\n",
    "        return scraped_content\n",
    "    \n",
    "    for i, url in enumerate(urls[:max_sites]):\n",
    "        try:\n",
    "            \n",
    "            response = requests.get(url, timeout=15, headers={\n",
    "                'User-Agent': 'StateGraphRAGAgent/1.0'\n",
    "            })\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):\n",
    "                    element.decompose()\n",
    "                \n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "                \n",
    "                if len(text) > 50000: \n",
    "                    text = text[:50000] + \"...\"\n",
    "                \n",
    "                if text and len(text) > 100:\n",
    "                    scraped_content.append(f\"Content from {url}:\\n{text}\")\n",
    "            else:\n",
    "                print(f\"❌ HTTP {response.status_code} from {url}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to scrape {url}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return scraped_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74616c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_rag(state):\n",
    "    search_data = []\n",
    "        \n",
    "    for tool_name, results in state[\"search_results\"].items():\n",
    "        if results:\n",
    "            if isinstance(results, dict) and 'results' in results:\n",
    "                for item in results['results']:\n",
    "                    content = item.get('content', '')\n",
    "                    title = item.get('title', '')\n",
    "                    url = item.get('url', '')\n",
    "                    \n",
    "                    if content:\n",
    "                        formatted_content = f\"Title: {title}\\nURL: {url}\\nContent: {content}\"\n",
    "                        search_data.append(f\"Source ({tool_name}): {formatted_content}\")\n",
    "                        \n",
    "            elif isinstance(results, str):\n",
    "                search_data.append(f\"Source ({tool_name}): {results}\")\n",
    "                \n",
    "    if search_data:\n",
    "        combined_text = \"\\n\\n\".join(search_data)\n",
    "        rag_pipeline.add_text_documents(combined_text)\n",
    "    \n",
    "    try:\n",
    "        scraped_content = scrape_urls_from_results(state[\"search_results\"], max_sites=5)\n",
    "        if scraped_content:\n",
    "            for content in scraped_content:\n",
    "                rag_pipeline.add_text_documents(content)\n",
    "        else:\n",
    "            print(\"⚠️ No websites were scraped\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Scraping error: {e}\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "112dbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_respond(state: AgentState):\n",
    "    query = state[\"query\"]\n",
    "    rag_results = rag_pipeline.retrieve(query, k=5)\n",
    "    \n",
    "    if rag_results:\n",
    "        prompt = f\"Answer this question based on the information: {query}\\n\\nInformation: {rag_results}\"\n",
    "        try:\n",
    "            response = model.invoke([HumanMessage(content=prompt)])\n",
    "            final_answer = response.content\n",
    "        except Exception as e:\n",
    "            final_answer = f\"Error: {e}\"\n",
    "    else:\n",
    "        final_answer = \"No information found.\"\n",
    "    \n",
    "    state[\"rag_results\"] = rag_results\n",
    "    state[\"final_answer\"] = final_answer\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a8b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"extract_query\", extract_query)\n",
    "workflow.add_node(\"tavily_search\", Tavily_search)\n",
    "workflow.add_node(\"duckduckgo_search\", Duckduckgo_search)\n",
    "workflow.add_node(\"wikipedia_search\", Wikipedia_search)\n",
    "workflow.add_node(\"store_in_rag\", store_in_rag)\n",
    "workflow.add_node(\"retrieve_and_respond\", retrieve_and_respond)\n",
    "\n",
    "workflow.set_entry_point(\"extract_query\")\n",
    "        \n",
    "workflow.add_edge(\"extract_query\", \"tavily_search\")\n",
    "workflow.add_conditional_edges(\n",
    "            \"tavily_search\",\n",
    "            check_search_success,\n",
    "            {\n",
    "                \"success\": \"store_in_rag\",\n",
    "                \"failure\": \"duckduckgo_search\"\n",
    "            }\n",
    "        )\n",
    "workflow.add_conditional_edges(\n",
    "            \"duckduckgo_search\",\n",
    "            check_search_success,\n",
    "            {\n",
    "                \"success\": \"store_in_rag\",\n",
    "                \"failure\": \"wikipedia_search\"\n",
    "            }\n",
    "        )\n",
    "workflow.add_conditional_edges(\n",
    "            \"wikipedia_search\",\n",
    "            check_search_success,\n",
    "            {\n",
    "                \"success\": \"store_in_rag\",\n",
    "                \"failure\": \"retrieve_and_respond\" \n",
    "            }\n",
    "        )\n",
    "workflow.add_edge(\"store_in_rag\", \"retrieve_and_respond\")\n",
    "workflow.add_edge(\"retrieve_and_respond\", END)\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a683105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=question)],\n",
    "        \"query\": \"\",\n",
    "        \"search_results\": {},\n",
    "        \"current_search_tool\": \"\",\n",
    "        \"rag_results\": \"\",\n",
    "        \"final_answer\": \"\"\n",
    "    }\n",
    "    \n",
    "    result = app.invoke(initial_state)\n",
    "    return result[\"final_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02a59a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who won FIFA World Cup 2022?\n",
      "Tavily search successful for: Who won FIFA World Cup 2022?\n",
      "Answer: Argentina won the 2022 FIFA World Cup, defeating France in the final match. This was Argentina's third World Cup victory.\n"
     ]
    }
   ],
   "source": [
    "question = \"Who won FIFA World Cup 2022?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {ask_question(question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18efc4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
