{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ab3806",
   "metadata": {},
   "source": [
    "## Task 2 -- Web scraper + RAG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30ea04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-huggingface langchain-cohere sentence-transformers langgraph \"langchain[google-genai]\" langchain_tavily langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c9a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, time, requests\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_cohere import ChatCohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ae3e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "TAVILY_API_KEY = os.environ[\"TAVILY_API_KEY\"]\n",
    "SERPAPI_KEY = os.environ[\"SERPAPI_KEY\"]\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=GEMINI_API_KEY, \n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad525606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tavily Agent\n",
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "tavily_search_tool = TavilySearch(\n",
    "    max_results=5,\n",
    "    topic=\"general\",\n",
    "    api_key=TAVILY_API_KEY\n",
    ")\n",
    "\n",
    "tavily_agent = create_react_agent(llm, [tavily_search_tool])\n",
    "\n",
    "# 2. SerpAPI Agent\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "serp = SerpAPIWrapper(\n",
    "    serpapi_api_key=SERPAPI_KEY\n",
    ")\n",
    "serp_tool = Tool(\n",
    "    name=\"serpapi-search\",\n",
    "    func=serp.run,\n",
    "    description=\"Search engine powered by SerpAPI\"\n",
    ")\n",
    "\n",
    "serp_agent = create_react_agent(llm, [serp_tool])\n",
    "\n",
    "# 3. DuckDuckGO Agent\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "duckduckgo_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "ddg_agent = create_react_agent(llm, [duckduckgo_tool])\n",
    "\n",
    "\n",
    "# Combined Agent\n",
    "agent = create_react_agent(llm, [tavily_search_tool, duckduckgo_tool, serp_tool])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17970e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "GOOGLE_API_KEY = os.environ[\"GEMINI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0df0bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "def scrape_with_rag(urls, query: str):\n",
    "    \"\"\"Scrape URLs, build FAISS index, run RAG for query.\"\"\"\n",
    "    docs = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            loader = WebBaseLoader(url)\n",
    "            docs.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {url}: {e}\")\n",
    "\n",
    "    if not docs:\n",
    "        return \" No content scraped.\"\n",
    "\n",
    "    # Chunk text\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=150)\n",
    "    splits = splitter.split_documents(docs)\n",
    "\n",
    "    # Temporary FAISS index\n",
    "    db = FAISS.from_documents(splits, embeddings)\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "    # RetrievalQA pipeline\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\"\n",
    "    )\n",
    "    return qa_chain.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "febdee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_query(user_input: str, use_rag=False):\n",
    "    \"\"\"Query multi-agent with optional RAG scraping fallback.\"\"\"\n",
    "    print(f\"\\n## Query: {user_input}\\n\")\n",
    "\n",
    "    try:\n",
    "        urls = []  # Collect URLs mentioned\n",
    "        answer = None\n",
    "\n",
    "        # Step 1: Run search\n",
    "        for step in agent.stream({\"messages\": user_input}, stream_mode=\"values\"):\n",
    "            msg = step[\"messages\"][-1]\n",
    "            msg.pretty_print()\n",
    "\n",
    "            if \"http\" in msg.content:\n",
    "                urls.extend([word for word in msg.content.split() if word.startswith(\"http\")])\n",
    "\n",
    "        # Step 2: Decide if RAG is needed\n",
    "        if use_rag and urls:\n",
    "            print(\"\\n### Using RAG on scraped sites...\\n\")\n",
    "            answer = scrape_with_rag(urls[:5], user_input)\n",
    "            print(answer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n**Primary agent failed:** {e}\\n\")\n",
    "        try:\n",
    "            print(\"### Fallback: DuckDuckGo Search\\n\")\n",
    "            print(duckduckgo_tool.run(user_input))\n",
    "        except Exception as e2:\n",
    "            print(f\"**All search methods failed:** {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "158103e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Query: What is the difference between LangChain and LlamaIndex?\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the difference between LangChain and LlamaIndex?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (f4cc91d6-e0c4-4803-9eac-5f7c97a50b05)\n",
      " Call ID: f4cc91d6-e0c4-4803-9eac-5f7c97a50b05\n",
      "  Args:\n",
      "    query: difference between LangChain and LlamaIndex\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"difference between LangChain and LlamaIndex\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://stackoverflow.com/questions/76990736/differences-between-langchain-llamaindex\", \"title\": \"Differences between Langchain & LlamaIndex [closed]\", \"content\": \"You'll be fine with just LangChain, however, LlamaIndex is optimized for indexing, and retrieving data. Retrieval-Augmented Generation (or RAG) is an architecture used to help large language models like GPT-4 provide better responses by using relevant information from additional sources and reducing the chances that an LLM will leak sensitive data, or ‘hallucinate’ incorrect or misleading information. If your app requires indexing and retrieval capabilities, and while you'll be just fine using LangChain (as it can handle that as well) I recommend integrating with LlamaIndex since it is optimized for that task and it is generally easier to ingest data using all the plugins and data connectors. It provides tools for loading, processing, and indexing data, as well as for interacting with LLMs. Langchain is also more flexible than LlamaIndex, allowing users to customize the behavior of their applications.\", \"score\": 0.9363841, \"raw_content\": null}, {\"url\": \"https://www.ibm.com/think/topics/llamaindex-vs-langchain\", \"title\": \"Llamaindex vs Langchain: What's the difference? - IBM\", \"content\": \"LlamaIndex focuses on indexing, data ingestion and information retrieval from text-based data sources, making it ideal for simpler workflows and straightforward AI applications. RAG systems respond to user queries by _retrieving_ relevant information from designated data sources in real time, then _augmenting_ the LLM’s _generative_ capabilities for better answers. While LlamaIndex shines when querying databases to retrieve relevant information, LangChain’s broader flexibility allows for a wider variety of use cases, especially when chaining models and tools into complex workflows. *   **Complex queries and data structures:** While LlamaIndex is built for semantic similarity, LangChain allows users to combine search techniques, such as by adding keyword search. IBM watsonx is powered by the latest AI models to intelligently process conversations and provide help whenever and wherever you may need it.\", \"score\": 0.93338853, \"raw_content\": null}, {\"url\": \"https://www.analyticsvidhya.com/blog/2024/11/langchain-vs-llamaindex/\", \"title\": \"Comparison Between LangChain and LlamaIndex - Analytics Vidhya\", \"content\": \"LlamaIndex (formerly known as GPT Index)is a framework for building context-augmented generative AI applications with LLMs includingagentsandworkflows.Its primary focus is on ingesting, structuring, and accessing private or domain-specific data. A. LangChain focuses on building complex workflows and interactive applications (e.g., chatbots, task automation), while LlamaIndex specializes in efficient search and retrieval from large datasets using vectorized embeddings. Generative AI| DeepSeek| OpenAI Agent SDK| LLM Applications using Prompt Engineering| DeepSeek from Scratch| Stability.AI| SSM & MAMBA| RAG Systems using LlamaIndex| Building LLMs for Code| Python| Microsoft Excel| Machine Learning| Deep Learning| Mastering Multimodal RAG| Introduction to Transformer Model| Bagging & Boosting| Loan Prediction| Time Series Forecasting| Tableau| Business Analytics| Vibe Coding in Windsurf| Model Deployment using FastAPI| Building Data Analyst AI Agent| Getting started with OpenAI o3-mini| Introduction to Transformers and Attention Mechanisms\", \"score\": 0.92604345, \"raw_content\": null}, {\"url\": \"https://blog.n8n.io/llamaindex-vs-langchain/\", \"title\": \"LlamaIndex vs. LangChain: Which RAG Tool is Right for You?\", \"content\": \"| Complex reasoning systems,  multi-agent applications,  applications requiring  integration with multiple  tools and APIs. You can  also use LangChain for  RAG workflows. Both LlamaIndex and LangChain are powerful frameworks for building LLM-powered applications, particularly those leveraging RAG. Integrate LlamaIndex's query engine as a tool within your LangChain workflow, allowing you to retrieve relevant information from your indexed data. Haystack is the best choice for search-heavy RAG applications, LlamaIndex excels at indexing and querying large datasets, and LangChain is ideal for orchestrating complex LLM workflows that involve both retrieval and external integrations. Use LangChain for complex workflows and multi-step logic, LlamaIndex for efficient data retrieval, and Hugging Face for accessing and fine-tuning pre-trained LLMs across a wide range of tasks.\", \"score\": 0.902687, \"raw_content\": null}, {\"url\": \"https://www.reddit.com/r/LangChain/comments/1bbog83/langchain_vs_llamaindex/\", \"title\": \"LangChain vs LlamaIndex - Reddit\", \"content\": \"LangChain vs LlamaIndex : r/LangChain Skip to main contentLangChain vs LlamaIndex : r/LangChain Open menu Open navigationGo to Reddit Home r/LangChain A chip A close button Get App Get the Reddit app Log InLog in to Reddit Expand user menu Open settings menu Go to LangChain r/LangChain r/LangChain LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. It is available for Python and Javascript at https://www.langchain.com/. Reply reply } Share Share FoxyFreak47 •1y ago I will detail out my experience - Langchain started as a whole LLM framework and continues to be so. Langchain is much better equipped and all-rounded in terms of utilities that it provides under one roof Llama-index started as a mega-library for data connectors.\", \"score\": 0.8989177, \"raw_content\": null}], \"response_time\": 1.42, \"request_id\": \"bc086853-3457-4263-920d-dba578d62fe4\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangChain and LlamaIndex are both frameworks for building applications with large language models (LLMs), especially those using Retrieval-Augmented Generation (RAG). However, they have different primary focuses:\n",
      "\n",
      "*   **LlamaIndex** specializes in **data ingestion, indexing, and retrieval** from various data sources. It's optimized for efficiently querying databases and retrieving relevant information to augment LLM responses. This makes it ideal for simpler workflows and applications where the main goal is to get information from your own data.\n",
      "\n",
      "*   **LangChain** is a more **general-purpose framework** for building complex LLM workflows and interactive applications. While it can handle RAG, its strength lies in its flexibility to chain together different models, tools, and APIs to create more sophisticated applications like chatbots or task automation. It allows for more customization and combining various search techniques.\n",
      "\n",
      "In essence, if your primary need is to efficiently index and retrieve information from your data for an LLM, LlamaIndex is a strong choice. If you need to build more complex applications that involve orchestrating multiple LLM interactions, external tools, and diverse workflows, LangChain offers greater flexibility. You can even integrate LlamaIndex's query engine as a tool within a LangChain workflow to leverage both frameworks' strengths.\n",
      "\n",
      "## Query: Summarize LangChain vs LlamaIndex from official docs.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Summarize LangChain vs LlamaIndex from official docs.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (c5dbd0a9-2d45-4317-a515-a1dd17e7f697)\n",
      " Call ID: c5dbd0a9-2d45-4317-a515-a1dd17e7f697\n",
      "  Args:\n",
      "    query: LangChain official documentation summary\n",
      "  tavily_search (3908c483-ac9e-4667-a724-bfb2e078300b)\n",
      " Call ID: 3908c483-ac9e-4667-a724-bfb2e078300b\n",
      "  Args:\n",
      "    query: LlamaIndex official documentation summary\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"LlamaIndex official documentation summary\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://docs.llamaindex.ai/en/stable/api_reference/indices/summary/\", \"title\": \"Summary - LlamaIndex\", \"content\": \"The summary index is a simple data structure where nodes are stored in a sequence. During index construction, the document texts are chunked up, converted to\", \"score\": 0.78631157, \"raw_content\": null}, {\"url\": \"https://docs.llamaindex.ai/en/stable/api_reference/indices/document_summary/\", \"title\": \"Document summary - LlamaIndex\", \"content\": \"DocumentSummaryIndex. Document Summary Index. A response synthesizer for generating summaries. The query to use to generate the summary for each document.\", \"score\": 0.76490957, \"raw_content\": null}, {\"url\": \"https://docs.llamaindex.ai/en/v0.10.17/examples/index_structs/doc_summary/DocSummary.html\", \"title\": \"Document Summary Index - LlamaIndex v0.10.17\", \"content\": \"The document summary index will extract a summary from each document and store that summary, as well as all nodes corresponding to the document.\", \"score\": 0.7077487, \"raw_content\": null}, {\"url\": \"https://docs.llamaindex.ai/en/v0.10.23/examples/index_structs/doc_summary/DocSummary/\", \"title\": \"Document Summary Index - LlamaIndex\", \"content\": \"The document summary index will extract a summary from each document and store that summary, as well as all nodes corresponding to the document. Retrieval can\", \"score\": 0.69622743, \"raw_content\": null}, {\"url\": \"https://docs.llamaindex.ai/\", \"title\": \"LlamaIndex - LlamaIndex\", \"content\": \"LlamaIndex LlamaIndex LlamaIndex LlamaIndex is the leading framework for building LLM-powered agents over your data with LLMs and workflows. Agents are LLM-powered knowledge assistants that use tools to perform tasks like research, data extraction, and more. LlamaIndex provides a framework for building agents including the ability to use RAG pipelines as one of many tools to complete a task. Workflows are multi-step processes that combine one or more agents, data connectors, and other tools to complete a task. LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code.\", \"score\": 0.6829338, \"raw_content\": null}], \"response_time\": 1.63, \"request_id\": \"3cd760f2-4771-4b34-9cf5-aa77559f29cc\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "LangChain is an open-source framework for developing applications powered by large language models (LLMs). It provides tools for building, managing, and deploying long-running, stateful agents, and offers a platform for debugging, testing, and monitoring AI applications. LangChain focuses on the cognitive architecture of applications, including chains, agents, and retrieval strategies.\n",
      "\n",
      "LlamaIndex is a framework for building LLM-powered agents over your data. It helps in creating knowledge assistants that use tools for tasks like research and data extraction. LlamaIndex provides a high-level API for data ingestion and querying, and it supports various context-augmentation use cases, from prototyping to production. It also features a document summary index that extracts and stores summaries from documents.\n"
     ]
    }
   ],
   "source": [
    "# Normal search only\n",
    "safe_query(\"What is the difference between LangChain and LlamaIndex?\")\n",
    "\n",
    "# Search + RAG scraping\n",
    "safe_query(\"Summarize LangChain vs LlamaIndex from official docs.\", use_rag=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83c39e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Query: Explain the working of a transformer\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain the working of a transformer\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (480e13fb-e66c-4e5c-ac65-80562a761a0c)\n",
      " Call ID: 480e13fb-e66c-4e5c-ac65-80562a761a0c\n",
      "  Args:\n",
      "    query: how does a transformer model work\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"how does a transformer model work\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://blogs.nvidia.com/blog/what-is-a-transformer-model/\", \"title\": \"What Is a Transformer Model? | NVIDIA Blogs\", \"content\": \"# What Is a Transformer Model? transformer model A transformer model is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence. ## **What Can Transformer Models Do?** Example of a transformer model and self-attention Attention is so key to transformers the Google researchers almost used the term as the name for their 2017 model. “Just as AI language models can learn the relationships between words in a sentence, our aim is that neural networks trained on molecular structure data will be able to learn the relationships between atoms in real-world molecules,” said Ola Engkvist, head of molecular AI, discovery sciences and R&D at AstraZeneca, when the work was announced last year.\", \"score\": 0.8846886, \"raw_content\": null}, {\"url\": \"https://www.datacamp.com/tutorial/how-transformers-work\", \"title\": \"How Transformers Work: A Detailed Exploration of ... - DataCamp\", \"content\": \"Explore the architecture of Transformers, the models that have revolutionized data handling through self-attention mechanisms. Characterized by their unique attention mechanisms and parallel processing abilities, Transformer models stand as a testament to the innovative leaps in understanding and generating human language with an accuracy and efficiency previously unattainable. Transformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead of using recurrence, the Transformer model is completely based on the Attention mechanism. Since Transformers do not have a recurrence mechanism like RNNs, they use positional encodings added to the input embeddings to provide information about the position of each token in the sequence. Here, the outputs from the encoder take on the roles of both queries and keys, while the outputs from the first multi-headed attention layer of the decoder serve as values.\", \"score\": 0.8732259, \"raw_content\": null}, {\"url\": \"https://www.ibm.com/think/topics/transformer-model\", \"title\": \"What is a Transformer Model? - IBM\", \"content\": \"Broadly speaking, a transformer model’s attention layers assess and use the specific context of each part of a data sequence in 4 steps: Transformer modelssuch asrelational databasesgenerate _query, key_ and _value vectors_ for each part of a data sequence,and use them to compute attention weights through a series of matrix multiplications. To generate query and key vectors to feed into the transformer’s attention layers, the model needs an initial, contextless vector embedding for each token. With _positional encoding,_ the model adds a vector of values to each token’s embedding, derived from its relative position, before the input enters the attention mechanism. To address this, transformer models often balance the contextual information provided by the attention mechanism with the original semantic meaning of each token.\", \"score\": 0.8488849, \"raw_content\": null}, {\"url\": \"https://huggingface.co/learn/llm-course/en/chapter1/4\", \"title\": \"How do Transformers work? - Hugging Face LLM Course\", \"content\": \"1. Transformer models In this section, we will take a look at the architecture of Transformer models and dive deeper into the concepts of attention, encoder-decoder architecture, and more. *   **June 2018**: GPT, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results Transformers are language models To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task. This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model — one as close as possible to the task you have at hand — and fine-tune it. As we dive into Transformer models in this course, you’ll see mentions of _architectures_ and _checkpoints_ as well as _models_.\", \"score\": 0.82795376, \"raw_content\": null}, {\"url\": \"https://aws.amazon.com/what-is/transformers-in-artificial-intelligence/\", \"title\": \"What are Transformers? - Transformers in Artificial Intelligence ...\", \"content\": \"How can AWS support your transformer model requirements? For example, consider this input sequence: \\\"What is the color of the sky?\\\" The transformer model uses an internal mathematical representation that identifies the relevancy and relationship between the words color, sky, and blue. The encoder reads and processes the entire input data sequence, such as an English sentence, and transforms it into a compact mathematical representation. This parallelization enables much faster training times and the ability to handle much longer sequences than RNNs. The self-attention mechanism in transformers also enables the model to consider the entire data sequence simultaneously. How can AWS support your transformer model requirements? **Amazon Web Services (AWS) offers the following AI/ML services that you can use for your transformer models requirements.**\", \"score\": 0.7756902, \"raw_content\": null}], \"response_time\": 1.4, \"request_id\": \"f74c8951-b9ad-4cef-8dc1-e08d1237538b\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "A Transformer model is a neural network architecture that excels at processing sequential data, like text. Unlike older models that processed data sequentially, Transformers use a mechanism called \"self-attention\" to understand the context and meaning of each part of the data simultaneously.\n",
      "\n",
      "Here's a simplified breakdown of how they work:\n",
      "\n",
      "1.  **Input Processing:**\n",
      "    *   Each word (or \"token\") in the input sequence is converted into a numerical representation called an \"embedding.\"\n",
      "    *   To account for the order of words, \"positional encodings\" are added to these embeddings. This gives the model information about where each word is located in the sequence.\n",
      "\n",
      "2.  **Attention Mechanism:**\n",
      "    *   The core of a Transformer is its attention mechanism. For each word, the model generates three vectors: a \"query,\" a \"key,\" and a \"value.\"\n",
      "    *   These vectors are used to calculate \"attention weights,\" which determine how much focus the model should place on other words in the sequence when processing a particular word. For example, when processing the word \"it\" in a sentence, the attention mechanism helps the model understand whether \"it\" refers to a \"cat\" or a \"dog\" mentioned earlier.\n",
      "    *   This process allows the model to consider the entire sequence at once, identifying relationships between words regardless of their distance.\n",
      "\n",
      "3.  **Encoder-Decoder Architecture (often):**\n",
      "    *   Many Transformers use an encoder-decoder structure.\n",
      "    *   The **encoder** reads and processes the input sequence, transforming it into a compact mathematical representation that captures the relationships between words.\n",
      "    *   The **decoder** then uses this representation to generate an output sequence (e.g., translating a sentence into another language or generating a response).\n",
      "\n",
      "4.  **Parallel Processing:**\n",
      "    *   Because Transformers don't rely on sequential processing like older recurrent neural networks (RNNs), they can process data in parallel. This significantly speeds up training and allows them to handle much longer sequences.\n",
      "\n",
      "In essence, Transformers learn context by tracking relationships within the data, allowing them to understand and generate human language with high accuracy and efficiency.\n"
     ]
    }
   ],
   "source": [
    "safe_query(\"Explain the working of a transformer\", use_rag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1493b957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Query: Explain the working of a transformer\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Explain the working of a transformer\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  tavily_search (c4e0c07f-afea-4595-a60c-8d47a9e41286)\n",
      " Call ID: c4e0c07f-afea-4595-a60c-8d47a9e41286\n",
      "  Args:\n",
      "    query: how does a transformer model work\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: tavily_search\n",
      "\n",
      "{\"query\": \"how does a transformer model work\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://blogs.nvidia.com/blog/what-is-a-transformer-model/\", \"title\": \"What Is a Transformer Model? | NVIDIA Blogs\", \"content\": \"# What Is a Transformer Model? transformer model A transformer model is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence. ## **What Can Transformer Models Do?** Example of a transformer model and self-attention Attention is so key to transformers the Google researchers almost used the term as the name for their 2017 model. “Just as AI language models can learn the relationships between words in a sentence, our aim is that neural networks trained on molecular structure data will be able to learn the relationships between atoms in real-world molecules,” said Ola Engkvist, head of molecular AI, discovery sciences and R&D at AstraZeneca, when the work was announced last year.\", \"score\": 0.8846886, \"raw_content\": null}, {\"url\": \"https://www.datacamp.com/tutorial/how-transformers-work\", \"title\": \"How Transformers Work: A Detailed Exploration of ... - DataCamp\", \"content\": \"Explore the architecture of Transformers, the models that have revolutionized data handling through self-attention mechanisms. Characterized by their unique attention mechanisms and parallel processing abilities, Transformer models stand as a testament to the innovative leaps in understanding and generating human language with an accuracy and efficiency previously unattainable. Transformers were inspired by the encoder-decoder architecture found in RNNs. However, Instead of using recurrence, the Transformer model is completely based on the Attention mechanism. Since Transformers do not have a recurrence mechanism like RNNs, they use positional encodings added to the input embeddings to provide information about the position of each token in the sequence. Here, the outputs from the encoder take on the roles of both queries and keys, while the outputs from the first multi-headed attention layer of the decoder serve as values.\", \"score\": 0.8732259, \"raw_content\": null}, {\"url\": \"https://www.ibm.com/think/topics/transformer-model\", \"title\": \"What is a Transformer Model? - IBM\", \"content\": \"Broadly speaking, a transformer model’s attention layers assess and use the specific context of each part of a data sequence in 4 steps: Transformer modelssuch asrelational databasesgenerate _query, key_ and _value vectors_ for each part of a data sequence,and use them to compute attention weights through a series of matrix multiplications. To generate query and key vectors to feed into the transformer’s attention layers, the model needs an initial, contextless vector embedding for each token. With _positional encoding,_ the model adds a vector of values to each token’s embedding, derived from its relative position, before the input enters the attention mechanism. To address this, transformer models often balance the contextual information provided by the attention mechanism with the original semantic meaning of each token.\", \"score\": 0.8488849, \"raw_content\": null}, {\"url\": \"https://huggingface.co/learn/llm-course/en/chapter1/4\", \"title\": \"How do Transformers work? - Hugging Face LLM Course\", \"content\": \"1. Transformer models In this section, we will take a look at the architecture of Transformer models and dive deeper into the concepts of attention, encoder-decoder architecture, and more. *   **June 2018**: GPT, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results Transformers are language models To perform fine-tuning, you first acquire a pretrained language model, then perform additional training with a dataset specific to your task. This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model — one as close as possible to the task you have at hand — and fine-tune it. As we dive into Transformer models in this course, you’ll see mentions of _architectures_ and _checkpoints_ as well as _models_.\", \"score\": 0.82795376, \"raw_content\": null}, {\"url\": \"https://aws.amazon.com/what-is/transformers-in-artificial-intelligence/\", \"title\": \"What are Transformers? - Transformers in Artificial Intelligence ...\", \"content\": \"How can AWS support your transformer model requirements? For example, consider this input sequence: \\\"What is the color of the sky?\\\" The transformer model uses an internal mathematical representation that identifies the relevancy and relationship between the words color, sky, and blue. The encoder reads and processes the entire input data sequence, such as an English sentence, and transforms it into a compact mathematical representation. This parallelization enables much faster training times and the ability to handle much longer sequences than RNNs. The self-attention mechanism in transformers also enables the model to consider the entire data sequence simultaneously. How can AWS support your transformer model requirements? **Amazon Web Services (AWS) offers the following AI/ML services that you can use for your transformer models requirements.**\", \"score\": 0.7756902, \"raw_content\": null}], \"response_time\": 0.76, \"request_id\": \"2faddb01-2aee-43ce-a64e-b511f37a79f2\"}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "A Transformer model is a neural network architecture that excels at processing sequential data, such as text. Unlike previous models like Recurrent Neural Networks (RNNs), Transformers do not rely on recurrence, which allows for much faster training and the ability to handle longer sequences.\n",
      "\n",
      "Here's a breakdown of how they work:\n",
      "\n",
      "1.  **Context and Meaning:** The fundamental idea behind Transformers is to learn the context and meaning of each element in a sequence by understanding its relationships with other elements. For example, in a sentence, it learns how words relate to each other.\n",
      "\n",
      "2.  **Self-Attention Mechanism:** This is the core innovation of Transformers. It allows the model to weigh the importance of different parts of the input sequence when processing each element. Imagine reading a sentence; when you focus on a particular word, your brain implicitly considers other words in the sentence that provide context. Self-attention mimics this by creating \"attention weights\" that determine how much focus each word should place on every other word in the sequence.\n",
      "\n",
      "3.  **Encoder-Decoder Architecture (often):** Transformers are often built with an encoder-decoder structure.\n",
      "    *   **Encoder:** The encoder processes the input sequence (e.g., an English sentence) and transforms it into a rich mathematical representation that captures the relationships between the elements.\n",
      "    *   **Decoder:** The decoder then uses this representation to generate an output sequence (e.g., a French translation).\n",
      "\n",
      "4.  **Query, Key, and Value Vectors:** To compute these attention weights, the model generates three types of vectors for each element in the sequence:\n",
      "    *   **Query (Q):** Represents the current element being processed.\n",
      "    *   **Key (K):** Represents all other elements in the sequence.\n",
      "    *   **Value (V):** Contains the actual information of all other elements.\n",
      "    The attention weights are calculated by comparing the Query of the current element with the Keys of all other elements. These weights are then used to create a weighted sum of the Value vectors, effectively allowing the model to \"attend\" to the most relevant parts of the input.\n",
      "\n",
      "5.  **Positional Encodings:** Since Transformers process the entire sequence in parallel (without recurrence), they need a way to understand the order of elements. Positional encodings are vectors added to the input embeddings that provide information about the relative or absolute position of each element in the sequence. This ensures that the model knows the sequence of words, even though it's processing them all at once.\n",
      "\n",
      "6.  **Parallel Processing:** A significant advantage of the attention mechanism is that it allows for parallel computation. Unlike RNNs, which process sequences step-by-step, Transformers can process all parts of the sequence simultaneously. This leads to much faster training times and the ability to handle much longer input sequences.\n",
      "\n",
      "In essence, a Transformer model learns to understand the context of each piece of information within a sequence by dynamically weighing its relationships with all other pieces of information, enabling it to perform complex tasks like language translation, text summarization, and question answering with high accuracy and efficiency.\n"
     ]
    }
   ],
   "source": [
    "safe_query(\"Explain the working of a transformer\", use_rag=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
